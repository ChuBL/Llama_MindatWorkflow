{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (6.0.2)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (3.4.1)\n",
      "Requirement already satisfied: pyvis in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (0.3.2)\n",
      "Requirement already satisfied: ipython>=5.3.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from pyvis) (8.28.0)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from pyvis) (3.1.4)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from pyvis) (3.3.0)\n",
      "Requirement already satisfied: networkx>=1.11 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from pyvis) (3.4.1)\n",
      "Requirement already satisfied: decorator in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis) (5.14.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis) (4.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from jinja2>=2.9.6->pyvis) (3.0.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from pexpect>4.3->ipython>=5.3.0->pyvis) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=5.3.0->pyvis) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from stack-data->ipython>=5.3.0->pyvis) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from stack-data->ipython>=5.3.0->pyvis) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from stack-data->ipython>=5.3.0->pyvis) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from asttokens>=2.1.0->stack-data->ipython>=5.3.0->pyvis) (1.16.0)\n",
      "Requirement already satisfied: folium in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (0.17.0)\n",
      "Requirement already satisfied: branca>=0.6.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from folium) (0.8.0)\n",
      "Requirement already satisfied: jinja2>=2.9 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from folium) (3.1.4)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from folium) (1.26.4)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from folium) (2.32.3)\n",
      "Requirement already satisfied: xyzservices in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from folium) (2024.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from jinja2>=2.9->folium) (3.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from requests->folium) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from requests->folium) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from requests->folium) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from requests->folium) (2024.8.30)\n",
      "Requirement already satisfied: ipympl in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (0.9.4)\n",
      "Requirement already satisfied: ipython-genutils in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from ipympl) (0.2.0)\n",
      "Requirement already satisfied: ipython<9 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from ipympl) (8.28.0)\n",
      "Requirement already satisfied: ipywidgets<9,>=7.6.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from ipympl) (8.1.5)\n",
      "Requirement already satisfied: matplotlib<4,>=3.4.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from ipympl) (3.9.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from ipympl) (1.26.4)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from ipympl) (10.4.0)\n",
      "Requirement already satisfied: traitlets<6 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from ipympl) (5.14.3)\n",
      "Requirement already satisfied: decorator in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from ipython<9->ipympl) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from ipython<9->ipympl) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from ipython<9->ipympl) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from ipython<9->ipympl) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from ipython<9->ipympl) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from ipython<9->ipympl) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from ipython<9->ipympl) (4.9.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from ipywidgets<9,>=7.6.0->ipympl) (0.2.2)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from ipywidgets<9,>=7.6.0->ipympl) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from ipywidgets<9,>=7.6.0->ipympl) (3.0.13)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from matplotlib<4,>=3.4.0->ipympl) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from matplotlib<4,>=3.4.0->ipympl) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from matplotlib<4,>=3.4.0->ipympl) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from matplotlib<4,>=3.4.0->ipympl) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from matplotlib<4,>=3.4.0->ipympl) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from matplotlib<4,>=3.4.0->ipympl) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from matplotlib<4,>=3.4.0->ipympl) (2.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from jedi>=0.16->ipython<9->ipympl) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from pexpect>4.3->ipython<9->ipympl) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython<9->ipympl) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib<4,>=3.4.0->ipympl) (1.16.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from stack-data->ipython<9->ipympl) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from stack-data->ipython<9->ipympl) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from stack-data->ipython<9->ipympl) (0.2.3)\n",
      "Requirement already satisfied: langchain-experimental in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (0.3.2)\n",
      "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain-experimental) (0.3.2)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.6 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain-experimental) (0.3.10)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.10.10)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.6.7)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.3 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.3)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.1.134)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.26.4)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.5.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.6->langchain-experimental) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.6->langchain-experimental) (24.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.6->langchain-experimental) (2.9.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.6->langchain-experimental) (4.12.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.15.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.6->langchain-experimental) (3.0.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain<0.4.0,>=0.3.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.4.0,>=0.3.6->langchain-experimental) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.4.0,>=0.3.6->langchain-experimental) (2.23.4)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2024.8.30)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (4.6.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.6)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.14.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.2.0)\n",
      "Requirement already satisfied: langgraph in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (0.2.35)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.2.39 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langgraph) (0.3.10)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langgraph) (2.0.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain-core<0.4,>=0.2.39->langgraph) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain-core<0.4,>=0.2.39->langgraph) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain-core<0.4,>=0.2.39->langgraph) (0.1.134)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain-core<0.4,>=0.2.39->langgraph) (24.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain-core<0.4,>=0.2.39->langgraph) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain-core<0.4,>=0.2.39->langgraph) (8.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain-core<0.4,>=0.2.39->langgraph) (4.12.2)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langgraph-checkpoint<3.0.0,>=2.0.0->langgraph) (1.1.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.2.39->langgraph) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph) (3.10.7)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.4,>=0.2.39->langgraph) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.4,>=0.2.39->langgraph) (2.23.4)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph) (4.6.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph) (1.0.6)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph) (3.10)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph) (2.2.3)\n",
      "Requirement already satisfied: langchain_openai in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (0.2.2)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.9 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain_openai) (0.3.10)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.40.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain_openai) (1.51.2)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain_openai) (0.8.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.9->langchain_openai) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.9->langchain_openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.9->langchain_openai) (0.1.134)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.9->langchain_openai) (24.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.9->langchain_openai) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.9->langchain_openai) (8.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.9->langchain_openai) (4.12.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (0.6.1)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (4.66.5)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain_openai) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain_openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain_openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain_openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.9->langchain_openai) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.9->langchain_openai) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.9->langchain_openai) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.4.0,>=0.3.9->langchain_openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.4.0,>=0.3.9->langchain_openai) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.2.3)\n",
      "Requirement already satisfied: openmindat in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (0.0.9)\n",
      "Requirement already satisfied: PyYAML in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from openmindat) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from openmindat) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from openmindat) (4.66.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from requests->openmindat) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from requests->openmindat) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from requests->openmindat) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/ollama2/lib/python3.12/site-packages (from requests->openmindat) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error --no-raise-exception\n",
    "# %%capture --no-stderr\n",
    "!pip install PyYAML==6.0.2\n",
    "!pip install matplotlib==3.9.2\n",
    "!pip install networkx==3.4.1\n",
    "!pip install pyvis==0.3.2\n",
    "!pip install folium==0.17.0\n",
    "!pip install ipympl==0.9.4\n",
    "!pip install langchain==0.3.3\n",
    "!pip install langchain-core==0.3.10\n",
    "!pip install langchain-community==0.3.2\n",
    "!pip install langchain-ollama==0.2.0\n",
    "!pip install langchain-experimental==0.3.2\n",
    "!pip install langgraph==0.2.35\n",
    "!pip install langchain-openai==0.2.2\n",
    "!pip install openmindat==0.0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MINDAT_API_KEY is set.\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "# Function to read YAML file\n",
    "def read_yaml(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            try:\n",
    "                content = yaml.safe_load(file)\n",
    "                return content\n",
    "            except yaml.YAMLError as exc:\n",
    "                print(f\"Error reading YAML file: {exc}\")\n",
    "                return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to write YAML file\n",
    "def write_yaml(file_path, data):\n",
    "    with open(file_path, 'w') as file:\n",
    "        try:\n",
    "            yaml.dump(data, file, default_flow_style=False)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(f\"Error writing YAML file: {exc}\")\n",
    "\n",
    "# Path to your YAML file\n",
    "yaml_file_path = '.apikey.yaml'\n",
    "\n",
    "# Read YAML file\n",
    "config = read_yaml(yaml_file_path)\n",
    "\n",
    "# Check if the config is loaded, otherwise prompt for input\n",
    "if config and 'api_key' in config:\n",
    "    os.environ[\"MINDAT_API_KEY\"] = config['api_key']\n",
    "else:\n",
    "    mindat_api_key = input(\"YAML file not found or 'mindat' key missing. Please enter your MINDAT_API_KEY: \")\n",
    "    os.environ[\"MINDAT_API_KEY\"] = mindat_api_key\n",
    "    \n",
    "    # Save the API key to YAML file\n",
    "    new_config = {'api_key': mindat_api_key}\n",
    "    write_yaml(yaml_file_path, new_config)\n",
    "    print(f\"API key saved to {yaml_file_path}.\")\n",
    "\n",
    "print(\"MINDAT_API_KEY is set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                ID              SIZE      MODIFIED    \n",
      "gene21d4/llama_3.1_instruct_8b_openmindat:latest    45ba34c46e72    4.9 GB    2 weeks ago    \n",
      "llama-3.1-instruct-8b-openmindat:latest             45ba34c46e72    4.9 GB    2 weeks ago    \n",
      "llama3.1:8b-instruct-q4_K_M                         46e0c10c039e    4.9 GB    2 weeks ago    \n",
      "llama3.1:latest                                     42182419e950    4.7 GB    3 weeks ago    \n",
      "llama3.2:3b                                         a80c4f17acd5    2.0 GB    4 weeks ago    \n",
      "nomic-embed-text:latest                             0a109f422b47    274 MB    4 weeks ago    \n",
      "mistral:latest                                      f974a74358d6    4.1 GB    4 weeks ago    \n",
      "llama3:latest                                       365c0bd3c000    4.7 GB    4 weeks ago    \n",
      "llama2:latest                                       78e26419b446    3.8 GB    4 weeks ago    \n",
      "gemma:latest                                        a72c7f4d0a15    5.0 GB    4 weeks ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# model = OllamaFunctions(\n",
    "#     model=\"llama3:8b\", \n",
    "#     format=\"json\"\n",
    "#     )\n",
    "\n",
    "# llm = ChatOllama(model=\"llama3.1:latest\", format='json')#, temperature=0)\n",
    "llm = ChatOllama(model=\"llama3.1:8b-instruct-q4_K_M\", format='json',  num_predict = 64)#, temperature=0)\n",
    "\n",
    "# llm = ChatOllama(model=\"llama3:8b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Agent Supervisor\n",
    "\n",
    "It will use function calling to choose the next worker node OR finish processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "import re\n",
    "\n",
    "# members = [\"Collector\", \"Network_Plotter\"]\n",
    "\n",
    "# members = [\"GEOMATERIAL_COLLECTOR\", \"LOCALITY_COLLECTOR\", \"HISTOGRAM_PLOTTER\", \"NETWORK_PLOTTER\", \"HEATMAP_PLOTTER\"]\n",
    "\n",
    "members = [\"GEOMATERIAL_COLLECTOR\", \"LOCALITY_COLLECTOR\", \"NETWORK_PLOTTER\", \"HEATMAP_PLOTTER\"]\n",
    "options = [\"FINISH\"] + members\n",
    "\n",
    "# Schema for structured response\n",
    "class NextDecision(BaseModel):\n",
    "    next: str = Field(description=\"Choose one from {options}\")\n",
    "    supervisor_reason: str = Field(description=\"The reason for choosing the next disicion. Please respond with 'next' and 'supervisor_reason' keys, no 'tool' or 'tool_input' needed\")\n",
    "    # model_construct: str = Field(description=\"placeholder\")\n",
    "\n",
    "\n",
    "supervisor_example1 = {\n",
    "    \"next\": \"GEOMATERIAL_COLLECTOR\",\n",
    "    \"supervisor_reason\": \"I need to collect the mineral dataset to execute the user request\"\n",
    "}\n",
    "\n",
    "supervisor_example2 = {\n",
    "    \"next\": \"NETWORK_PLOTTER\",\n",
    "    \"supervisor_reason\": \"The dataset is collected, and the user want to see the network visualization\"\n",
    "}\n",
    "\n",
    "supervisor_example3 = {\n",
    "    \"next\": \"FINISH\",\n",
    "    \"supervisor_reason\": \"The user request is fulfilled, or beyond the agent working abilities.\"\n",
    "}\n",
    "\n",
    "## Supervisor Router\n",
    "supervisor_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a supervisor managing a conversation between the workers: {members}. \\n\n",
    "    Respond with FINISH if the request is fulfilled. \\n\n",
    "    Following are some general steps for your references:\n",
    "    Case 1, \"User: i want the copper mineral datasets\", please go 1. GEOMATERIAL_COLLECTOR, 2. FINISH \\n\n",
    "    Case 2, \"User: i want the network of iron mineral\", please go 1. GEOMATERIAL_COLLECTOR, 2. NETWORK_PLOTTER, 3. FINISH.\\n\n",
    "    Case 3, \"User: i want the mineral heatmap for Brazil\", please go 1. LOCALITY_COLLECTOR, 2. HEATMAP_PLOTTER, 3. FINISH.\\n\n",
    "\n",
    "    Team members description:\n",
    "    - GEOMATERIAL_COLLECTOR: the geomaterial collector agent, should be called in the first place to obtain mineral dataset\\n\n",
    "    - LOCALITY_COLLECTOR: the mineral locality collector agent, should be called in the first place to obtain locality dataset\\n\n",
    "    - NETWORK_PLOTTER: the network visualization plotter agent, will plot the network, cannot plot without the dataset of GEOMATERIAL_COLLECTOR\\n\n",
    "    - HEATMAP_PLOTTER: the heatmap visualization plotter agent, will plot the heatmap, cannot plot without the dataset of GEOMATERIAL_COLLECTOR\\n\n",
    "\n",
    "    Your json answer in 'args' must include 'next' and \"supervisor_reason\" keys, \\n \n",
    "    Example1: \"i want you to plot network visualization for carbon and oxygen mineral but without Magnesium\", please call the tool with 'args' of {supervisor_example1}\\n\n",
    "    Example2: \"User: i want you to plot network visualization for carbon and oxygen mineral but without Magnesium, Step 0, Supervisor node: GEOMATERIAL_COLLECTOR, Step 1, Collector node: The Collector node successfully saved the dataset to /Users/blc/pyspace/Git_Mindat/mindatxllm/content/mindat_data/geomaterials_data.json\", please call the tool with 'args' of {supervisor_example2}\\n\n",
    "    For user requests beyond the team member abilities or mineral domains, simply go FINISH.\n",
    "    \"User: I want to order a bottle of water\", please call the tool with 'args' of  {supervisor_example3}\n",
    "\n",
    "    Please revise your response according to {errors} if present.\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Messages : {messages} \\n\n",
    "    Given the conversation above, who should act next?\n",
    "    Or should we FINISH? Select one of: {options}\n",
    "    \n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"members\",\"messages\",\"options\", \"errors\", \"supervisor_example1\", \"supervisor_example2\", \"supervisor_example3\"],\n",
    ")\n",
    "\n",
    "# llm = OllamaFunctions(model=\"llama3:8b\", format='json')\n",
    "\n",
    "structured_llm = llm.with_structured_output(NextDecision)\n",
    "supervisor_chain = supervisor_prompt | structured_llm\n",
    "\n",
    "# llm_with_tools = llm.bind_tools([NextDecision])\n",
    "# supervisor_chain = supervisor_prompt | llm_with_tools\n",
    "\n",
    "errors = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### supervisor test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error --no-raise-exception\n",
    "import re\n",
    "\n",
    "structured_llm = llm.with_structured_output(NextDecision)\n",
    "supervisor_chain = supervisor_prompt | structured_llm\n",
    "\n",
    "\n",
    "messages = \"\"\"User: i want you to plot network visualization for carbon and oxygen mineral but without Magnesium\n",
    "Step 0, Supervisor node: COLLECTOR\n",
    "Step 1, Collector node: The Collector node successfully saved the dataset to /Users/blc/pyspace/Git_Mindat/mindatxllm/content/mindat_data/geomaterials_data.json\"\"\"\n",
    "\n",
    "errors = ''\n",
    "retry_tolerance = 5\n",
    "\n",
    "def parse_required_keys_from_llm_response(llm_response):\n",
    "    # 定义所需的键\n",
    "    required_keys = [\"next\", \"supervisor_reason\"]\n",
    "    extracted_data = {}\n",
    "\n",
    "    # 使用正则表达式匹配所有键值对，匹配键和值为双引号或单引号包裹的内容\n",
    "    for key in required_keys:\n",
    "        pattern = rf'[\"]({key})[\"](?:\\s*:\\s*)[\"](.*?)[\"]'\n",
    "        match = re.search(pattern, llm_response, re.DOTALL)\n",
    "        if match:\n",
    "            extracted_data[match.group(1)] = match.group(2)\n",
    "\n",
    "    # 检查是否所有键都被提取\n",
    "    if len(extracted_data) == len(required_keys):\n",
    "        return json.loads(json.dumps(extracted_data))\n",
    "    else:\n",
    "        missing_keys = set(required_keys) - set(extracted_data.keys())\n",
    "        raise ValueError(f\"Missing keys in LLM response: {missing_keys}\")\n",
    "\n",
    "for retry_count in range(retry_tolerance):\n",
    "    print(f\"Retrying({retry_count + 1}/{retry_tolerance})\")\n",
    "    try:\n",
    "        result = supervisor_chain.invoke({\n",
    "            \"members\": members, \n",
    "            \"messages\": str(messages), \n",
    "            \"options\": options, \n",
    "            \"errors\": errors,\n",
    "            \"supervisor_example1\": supervisor_example1,\n",
    "            \"supervisor_example2\": supervisor_example2,\n",
    "            \"supervisor_example3\": supervisor_example3,\n",
    "        })\n",
    "\n",
    "        print(\"supervisor output: \", result)\n",
    "\n",
    "        result_next = result.next  # Access the 'next' attribute of the NextDecision object\n",
    "        if result_next.upper() in options:\n",
    "            break\n",
    "        else:\n",
    "            jargon_errors = f\"{result_next} not in {options}, please try again.\"\n",
    "            errors = jargon_errors\n",
    "            print(\"Jargon errors: \", jargon_errors)\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(\"Parsing...\")\n",
    "        print(e)\n",
    "        print(parse_required_keys_from_llm_response(str(e)))\n",
    "        # raise SystemExit(\"Terminating execution\")\n",
    "    except (KeyError, IndexError, AttributeError) as e:\n",
    "        error_type = type(e).__name__\n",
    "        errors = f\"{error_type}: {str(e)}\"\n",
    "        print(errors)\n",
    "\n",
    "print(\"supervisor test done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Action Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geomaterial Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.tools import tool\n",
    "# from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "# Import things that are needed generically\n",
    "\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "from openmindat import GeomaterialSearchRetriever\n",
    "import json, pprint\n",
    "\n",
    "\n",
    "# tavily_tool = TavilySearchResults(max_results=5)\n",
    "\n",
    "# This executes code locally, which can be unsafe\n",
    "# python_repl_tool = PythonREPLTool()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from typing import Annotated, List, Tuple, Union, Optional\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from openmindat import GeomaterialRetriever\n",
    "from pathlib import Path\n",
    "\n",
    "class GeomaterialQueryDict(BaseModel):\n",
    "    ima: Optional[bool] = Field(description=\"Only IMA-approved names, should be True by default\")\n",
    "    hardness_min: Optional[Union[float,str]] = Field(description=\"Mohs hardness from, leave as '' if necessary\")\n",
    "    hardness_max: Optional[Union[float,str]] = Field(description=\"Mohs hardness to, leave as '' if necessary\")\n",
    "    crystal_system: Optional[list[str]] = Field(description=\"Optional Crystal system (csystem): multiple choice (OR), Items Enum: 'Amorphous','Hexagonal','Icosahedral','Isometric','Monoclinic','Orthorhombic','Tetragonal','Triclinic','Trigonal', leave as [] if not required\")\n",
    "    elements_inc: Optional[str] = Field(description=\"Chemical elements must include, e.g., 'Fe,Cu', leave as '' if necessary\")\n",
    "    elements_exc: Optional[str] = Field(description=\"Chemical elements must exclude, e.g., 'Fe,Cu', leave as '' if necessary\")\n",
    "    expand: Optional[str] = Field(description=\"Expand the search scope with 'locality', leave as '' if necessary\")\n",
    "\n",
    "\n",
    "# class MindatCollectorInput(BaseModel):\n",
    "#     name: str =  Field(description=\"the name of the tool\")\n",
    "#     args: MindatQueryDict = Field(description=\"a json with key-value pairs for the querying dict\")\n",
    "\n",
    "\n",
    "# class MindatCollectorInput(BaseModel):\n",
    "#     query: MindatQueryDict = Field(description=\"\"\"Example dicts, all of the keys are optional, omit the keys without values if necessary:\n",
    "# {\n",
    "#     \"ima\": True,  # Only IMA-approved names, should be True by default\n",
    "#     \"hardness_min\": 1.0,  # Mohs hardness from 1, , leave as '' if necessary\n",
    "#     \"hardness_max\": 10.0,  # Mohs hardness to 10, , leave as '' if necessary\n",
    "#     \"crystal_system\": [\"Hexagonal\"],  # Hexagonal crystal system, , leave as []  if not required\n",
    "#     \"elements_inc\": \"Ag,H\",  # Must include Gold (Ag) and Hxygen (H), , leave as '' if necessary\n",
    "#     \"elements_exc\": \"Fe\",  # Exclude Iron (Fe), , leave as '' if necessary\n",
    "#     \"expand\": \"locality\", # Expand the search scope with 'locality', , leave as '' if necessary\n",
    "# }\n",
    "# \"\"\")\n",
    "\n",
    "def geomaterial_collector_function(query: dict):\n",
    "    print(\"====in geomaterial collecting function====\")\n",
    "    # if 'query_dict' in query:\n",
    "    #     query = query['query_dict']\n",
    "    # original_dict = query.dict()\n",
    "    original_dict = query\n",
    "    # import sys\n",
    "    # sys.exit()\n",
    "\n",
    "    # Filter out keys with empty values\n",
    "    filtered_dict = {key: value for key, value in original_dict.items() if value and value != '[]'}\n",
    "   \n",
    "    if 'expand' in filtered_dict:\n",
    "        filtered_dict.update({'page_size': 200})\n",
    "    # filtered_query = {k: v for k, v in filtered_dict.items() if v}\n",
    "    print(filtered_dict)\n",
    "\n",
    "    gr = GeomaterialRetriever()\n",
    "    gr._params.update(filtered_dict)\n",
    "    saving_path = Path(\"./mindat_data\").resolve()\n",
    "    file_name = \"geomaterial_data\"\n",
    "    gr.saveto(saving_path, file_name)\n",
    "    \n",
    "    file_path = Path(saving_path, file_name + '.json')\n",
    "\n",
    "    if geomaterial_checker(file_path):\n",
    "        return f\"The Geomaterial Collector node has successfully saved the dataset to {file_path}\"\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def geomaterial_checker(FILE_PATH: str):\n",
    "    '''Check if the download request is valid'''\n",
    "    with open(FILE_PATH, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        if 0 == len(data[\"results\"]):\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "# mindat_collect = StructuredTool.from_function(\n",
    "#     func=mindat_collector_function,\n",
    "#     name=\"MindatCollect\",\n",
    "#     description=\"\"\"useful for collecting mindat mineral information and saving as json, will return the file path.\n",
    "#     \"\"\",\n",
    "#     args_schema=MindatCollectorInput\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'elements_exc': 'Mg', 'elements_inc': 'C,Fe', 'expand': 'locality', 'ima': 'True', 'page_size': 200}\n"
     ]
    }
   ],
   "source": [
    "sample_dict = {'crystal_system': '[]', 'elements_exc': 'Mg', 'elements_inc': 'C,Fe', 'expand': 'locality', 'ima': 'True', 'page_size': 200}\n",
    "filtered_dict = {key: value for key, value in sample_dict.items() if value and value != '[]'}\n",
    "print(filtered_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_query1 = {\n",
    "  # \"name\": \"COLLECTOR\",\n",
    "  #\"args\": {\n",
    "    \"ima\": True,\n",
    "    \"elements_inc\": \"Cu\",\n",
    "    \"hardness_min\": \"\",\n",
    "    \"hardness_max\": \"\",\n",
    "    \"crystal_system\": [],\n",
    "    \"elements_exc\": \"\"\n",
    "  #}\n",
    "}\n",
    "example_query2 = {\n",
    "  # \"name\": \"COLLECTOR\",\n",
    "  #\"args\": {\n",
    "    \"ima\": True,\n",
    "    \"elements_inc\": \"Ag,H\",\n",
    "    \"hardness_min\": 1.0,\n",
    "    \"hardness_max\": 10.0,\n",
    "    \"crystal_system\": [\"Hexagonal\"],\n",
    "    \"elements_exc\": \"Fe\"\n",
    "  #}\n",
    "}\n",
    "example_query3 = {\n",
    "  # \"name\": \"COLLECTOR\",\n",
    "  #\"args\": {\n",
    "    \"ima\": True,\n",
    "    \"elements_inc\": \"Al\",\n",
    "    \"hardness_min\": \"\",\n",
    "    \"hardness_max\": \"\",\n",
    "    \"crystal_system\": [],\n",
    "    \"elements_exc\": \"Na\",\n",
    "    \"expand\": \"locality\"\n",
    "\n",
    "  #}\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "## collector chain\n",
    "geo_collector_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a mineral data collector for mindat api, collecting datasets for user purpose, e.g., data collecting, mineral visualization, etc.\n",
    "\n",
    "    Example dicts, all of the keys are optional \\n\n",
    "    Example 1: \"i want the copper mineral\" response: {example_query1} \\n\n",
    "    Example 2: \"I want the ima mineral with hardness from 3 to 5, in crystall hexagonal, must include silver and hydrogen, and no iron\" response: {example_query2} \\n\n",
    "    Example 3: \"Please plot the network for Aluminum minerals, but no Sodium involved\" response: {example_query3} Remember to expand with 'locality' for network visualization\\n\n",
    "    Remember use key 'expand' with value 'locality' for network visualization. Remember use key 'expand' with value 'locality' for network visualization. Remember use key 'expand' with value 'locality' for network visualization. \\n\n",
    "\n",
    "    Please use the given json example with \"name\" and \"args\" as keys\n",
    "\n",
    "    Pay attention to the error message if presents, revise your response accordingly.\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Messages : {messages} \\n\n",
    "    Errors: {errors}\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"messages\", \"errors\", \"example_query1\", \"example_query2\", \"example_query3\"],\n",
    ")\n",
    "\n",
    "# llm = OllamaFunctions(model=\"llama3:8b\", format='json')\n",
    "\n",
    "# collector_structured_llm = llm.with_structured_output(MindatQueryDict)\n",
    "# collector_chain = collector_prompt | collector_structured_llm\n",
    "\n",
    "geo_collector_llm_with_tools = llm.bind_tools([GeomaterialQueryDict])\n",
    "# collector_llm_with_tools = llm.bind_tools([MindatCollectorInput])\n",
    "geo_collector_chain = geo_collector_prompt | geo_collector_llm_with_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Geomaterial collector test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error --no-raise-exception\n",
    "sample_query = {'ima': True, 'elements_inc': 'C,O', 'elements_exc': 'Mg', 'expand': 'locality'}\n",
    "geomaterial_collector_function(sample_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error --no-raise-exception\n",
    "# healthy?\n",
    "# messages = \"\"\"User\n",
    "# plot the histogram of the elements distribution of the ima-approved mineral species with hardness between 3-5, in Hexagonal crystal system, must have Neodymium, but without sulfur\"\"\"\n",
    "\n",
    "# messages = 'I want the datasets of iron minerals but without oxygen'\n",
    "# messages = \"\"\"User: i want you to plot histogram visualization for carbon and oxygen mineral but without Magnesium\n",
    "# Step 0, Supervisor node: HISTOGRAM_PLOTTER\n",
    "# Step 1, Histogram plotter node: The file path was not a valid path, please collect the dataset first.\"\"\"\n",
    "messages = \"User: i want you to plot histogram visualization for carbon and oxygen mineral but without Magnesium\"\n",
    "errors = ''\n",
    "\n",
    "tolerance = 3\n",
    "for t in range(tolerance):\n",
    "    print(t)\n",
    "    try:\n",
    "        result = geo_collector_chain.invoke({\n",
    "                \"messages\": str(messages), \n",
    "                \"errors\": errors, \n",
    "                'example_query1': example_query1,\n",
    "                'example_query2': example_query2,\n",
    "                'example_query3': example_query3,\n",
    "                })\n",
    "        break\n",
    "    except (ValueError, IndexError, KeyError) as e:\n",
    "            error_type = type(e).__name__\n",
    "            errors = f\"{error_type}: {str(e)}\"\n",
    "            print(errors)\n",
    "    \n",
    "print(result)\n",
    "print(type(result))\n",
    "\n",
    "print(result.tool_calls[0]['args'])\n",
    "geomaterial_collector_function(result.tool_calls[0]['args'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Locality Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openmindat import LocalitiesRetriever\n",
    "\n",
    "class LocalityQueryDict(BaseModel):\n",
    "    country: str = Field(description=\"The country name\")\n",
    "    # txt: str = Field(description=\"The keywords to search for localities\")\n",
    "\n",
    "\n",
    "def locality_collector_function(query: dict):\n",
    "    print(\"====in locality collecting function====\")\n",
    "    # if 'query_dict' in query:\n",
    "    #     query = query['query_dict']\n",
    "    original_dict = query\n",
    "\n",
    "    filtered_dict = {\"country\": original_dict.get('country')}\n",
    "\n",
    "    if not filtered_dict.get('country'):\n",
    "        return False\n",
    "\n",
    "    print(filtered_dict)\n",
    "\n",
    "    lr = LocalitiesRetriever()\n",
    "    lr._params.update(filtered_dict)\n",
    "    saving_path = Path(\"./mindat_data\").resolve()\n",
    "    file_name = \"locality_data\"\n",
    "\n",
    "    lr.saveto(saving_path, file_name)\n",
    "\n",
    "    file_path = Path(saving_path, file_name + '.json')\n",
    "\n",
    "    if locality_checker(file_path):\n",
    "        return f\"The Locality Colector node has successfully saved the dataset to {file_path}\"\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def locality_checker(FILE_PATH: str):\n",
    "    '''Check if the download request is valid'''\n",
    "    with open(FILE_PATH, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        if 0 == len(data[\"results\"]):\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_loc_query1 = {\n",
    "  \"country\": \"Brazil\"\n",
    "}\n",
    "example_loc_query2 = {\n",
    "  \"country\": \"Canada\"\n",
    "}\n",
    "example_loc_query3 = {\n",
    "  # \"name\": \"COLLECTOR\",\n",
    "  #\"args\": {\n",
    "    \"country\": \"\"\n",
    "  #}\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "## collector chain\n",
    "loc_collector_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a mineral locality data collector for mindat api, collecting locality datasets for user purpose, e.g., data collecting, create heatmap visualization, etc.\\n\n",
    "\n",
    "    You should only retrieve the datasets for 1. locality datasets retrieval 2. heatmap visualization creation.\n",
    "\n",
    "    Example dicts, all of the keys are optional \\n\n",
    "    Example 1: \"i want the mineral records from Brazil\" response: {example_loc_query1} \\n\n",
    "    Example 2: \"plot the heatmap for Canada: {example_loc_query2} \\n\n",
    "    Example 3: \"Please plot the networdk for Norway\" response: {example_loc_query3} # Reject invalid network visualization request with empty country value\\n\n",
    "\n",
    "    Please use the given json example with \"name\" and \"args\" as keys\n",
    "\n",
    "    Pay attention to the error message if presents, revise your response accordingly.\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Messages : {messages} \\n\n",
    "    Errors: {errors}\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"messages\", \"errors\", \"example_loc_query1\", \"example_loc_query2\", \"example_loc_query3\"],\n",
    ")\n",
    "\n",
    "# llm = OllamaFunctions(model=\"llama3:8b\", format='json')\n",
    "\n",
    "# collector_structured_llm = llm.with_structured_output(MindatQueryDict)\n",
    "# collector_chain = collector_prompt | collector_structured_llm\n",
    "\n",
    "loc_collector_llm_with_tools = llm.bind_tools([LocalityQueryDict])\n",
    "# collector_llm_with_tools = llm.bind_tools([MindatCollectorInput])\n",
    "loc_collector_chain = loc_collector_prompt | loc_collector_llm_with_tools\n",
    "\n",
    "\n",
    "# mindat_locality_collect = StructuredTool.from_function(\n",
    "#     func=locality_collector_function,\n",
    "#     name=\"MindatLocalitiyCollect\",\n",
    "#     description=\"\"\"useful for collecting mindat locality information and saving as json, will return the file path.\n",
    "#     \"\"\",\n",
    "#     args_schema=LocalityQueryDict\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Locality collect test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error --no-raise-exception\n",
    "# healthy?\n",
    "# messages = \"\"\"User\n",
    "# plot the histogram of the elements distribution of the ima-approved mineral species with hardness between 3-5, in Hexagonal crystal system, must have Neodymium, but without sulfur\"\"\"\n",
    "\n",
    "# messages = 'I want the datasets of iron minerals but without oxygen'\n",
    "# messages = \"\"\"User: i want you to plot histogram visualization for carbon and oxygen mineral but without Magnesium\n",
    "# Step 0, Supervisor node: HISTOGRAM_PLOTTER\n",
    "# Step 1, Histogram plotter node: The file path was not a valid path, please collect the dataset first.\"\"\"\n",
    "# messages = \"User: i want you to plot histogram visualization for carbon and oxygen mineral but without Magnesium\"\n",
    "\n",
    "messages = \"User: i want you to plot network visualization for Korea\"\n",
    "\n",
    "\n",
    "errors = ''\n",
    "\n",
    "tolerance = 3\n",
    "for t in range(tolerance):\n",
    "    print(t)\n",
    "    try:\n",
    "        result = loc_collector_chain.invoke({\n",
    "                \"messages\": str(messages), \n",
    "                \"errors\": errors, \n",
    "                'example_loc_query1': example_loc_query1,\n",
    "                'example_loc_query2': example_loc_query2,\n",
    "                'example_loc_query3': example_loc_query3,\n",
    "                })\n",
    "        break\n",
    "    except (ValueError, IndexError, KeyError) as e:\n",
    "            error_type = type(e).__name__\n",
    "            errors = f\"{error_type}: {str(e)}\"\n",
    "            print(errors)\n",
    "    \n",
    "print(result)\n",
    "print(type(result))\n",
    "\n",
    "print(result.tool_calls[0]['args'])\n",
    "locality_collector_function(result.tool_calls[0]['args'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error --no-raise-exception\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class HistogramInput(BaseModel):\n",
    "    file_path: str = Field(description=\"Should be a json file path of json data, leave empty if not given.\")\n",
    "    scratch_pad: str = Field(description=\"Should be your comment, e.g., the file path is not given; the process works well, etc.\")\n",
    "\n",
    "def histogram_plot_function(file_path: str):\n",
    "    df = pd.read_json(file_path)\n",
    "    df = pd.json_normalize(df['results'])\n",
    "    # Explode the DataFrame to have each element on a separate row\n",
    "    df_exploded = df.explode('elements')\n",
    "\n",
    "    # Count the frequency of each element\n",
    "    element_counts = df_exploded['elements'].value_counts()\n",
    "\n",
    "    # Select the top 30 elements\n",
    "    top_30_elements = element_counts.head(30)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_30_elements.plot(kind='bar')\n",
    "    plt.title('Top 30 Frequent Elements Distribution')\n",
    "    plt.xlabel('Element')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    return \"Successfully plotted the required diagram.\"\n",
    "\n",
    "# pandas_plot = StructuredTool.from_function(\n",
    "#     func=pandas_plot_function,\n",
    "#     name=\"PandasPlot\",\n",
    "#     description=\"useful for plotting the element distributions of the mineral data.\",\n",
    "#     args_schema=PandasDFInput\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error --no-raise-exception\n",
    "## histogram chain\n",
    "histogram_plotter_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "    You are a mineral data histogram plotter. Your team member will collect the dataset and give you the file path to work with.\n",
    "    All you need is the file path. \n",
    "    \n",
    "    The visualization tool is included.\n",
    "\n",
    "    Your response should include two keys in json, \"file_path\" and \"scratch_pad\" \\n\n",
    "    If the file path to the dataset is present in the message, return it in file_path field. The visualization tool will process the file.\n",
    "    If the file path isn't presented in the message, respond with \"file_path\": \"\", then comment it in the \"scratch_pad\": \"The dataset file path is empty, please collect the dataset first.\"\n",
    "\n",
    "    Pay attention to the error message if presents, revise your response accordingly.\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Messages : {messages} \\n\n",
    "    Errors: {errors} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"messages\", \"errors\"],\n",
    ")\n",
    "\n",
    "# llm = OllamaFunctions(model=\"llama3:8b\", format='json')\n",
    "\n",
    "# histogram_plotter_structured_llm = llm.with_structured_output(HistogramInput)\n",
    "# histogram_chain = histogram_plotter_prompt | histogram_plotter_structured_llm\n",
    "\n",
    "histogram_plotter_llm_with_tools = llm.bind_tools([HistogramInput])\n",
    "histogram_chain = histogram_plotter_prompt | histogram_plotter_llm_with_tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Histogram test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error --no-raise-exception\n",
    "\n",
    "# messages = \"\"\"User\n",
    "# plot the histogram of the elements distribution of the ima-approved mineral species with hardness between 3-5, in Hexagonal crystal system, must have Neodymium, but without sulfur\"\"\"\n",
    "# messages = 'I want the datasets of iron minerals but without oxygen, the dataset has been saved in content/mindat_data/geomaterials_data.json'\n",
    "messages = 'I want the datasets of iron minerals but without oxygen, '\n",
    "errors = ''\n",
    "\n",
    "tolerance = 10\n",
    "for t in range(tolerance):\n",
    "    print(t)\n",
    "    try:\n",
    "        result = histogram_chain.invoke({\"messages\": messages, 'errors': errors})\n",
    "        break\n",
    "    except ValueError as e:\n",
    "        errors = str(e)\n",
    "        print(e)\n",
    "    \n",
    "print(result)\n",
    "print(type(result))\n",
    "\n",
    "# histogram_plot_function(result.tool_calls[0]['args']['file_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "from IPython.display import display, HTML, Image, IFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import webbrowser\n",
    "\n",
    "class NetworkVizInput(BaseModel):\n",
    "    file_path: str = Field(description=\"Should be a file path of json data, leave empty if not given.\")\n",
    "    scratch_pad: str = Field(description=\"Should be your comment, e.g., the file path is not given; the process works well, etc.\")\n",
    "\n",
    "def network_plot_function(file_path: str, top_n=50, display_method='new_window'):\n",
    "    # 定义颜色映射\n",
    "    color_map = {\n",
    "        1: 'red', 2: 'orange', 3: 'yellow', 4: 'green', 5: 'blue',\n",
    "        6: 'indigo', 7: 'violet', 8: 'purple', 9: 'brown', 10: 'grey', 11: 'black'\n",
    "    }\n",
    "\n",
    "    # 定义 Strunz 分类的图例\n",
    "    legend_html = \"\"\"\n",
    "    <div style=\"position: absolute; bottom: 50px; left: 10px; background-color: white; padding: 10px; border: 1px solid black; z-index: 9999;\">\n",
    "        <b>Strunz Classification - Primary Groups</b><br>\n",
    "        1. <span style=\"color: red;\">ELEMENTS</span><br>\n",
    "        2. <span style=\"color: orange;\">SULFIDES and SULFOSALTS</span><br>\n",
    "        3. <span style=\"color: yellow;\">HALIDES</span><br>\n",
    "        4. <span style=\"color: green;\">OXIDES</span><br>\n",
    "        5. <span style=\"color: blue;\">CARBONATES</span><br>\n",
    "        6. <span style=\"color: indigo;\">BORATES</span><br>\n",
    "        7. <span style=\"color: violet;\">SULFATES</span><br>\n",
    "        8. <span style=\"color: purple;\">PHOSPHATES, ARSENATES, VANADATES</span><br>\n",
    "        9. <span style=\"color: brown;\">SILICATES</span><br>\n",
    "        10. <span style=\"color: grey;\">ORGANIC COMPOUNDS</span><br>\n",
    "        11. <span style=\"color: black;\">Other</span><br>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "    # 加载 JSON 数据\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # 过滤数据以仅包括前 n 个矿物\n",
    "    filtered_data = data[\"results\"][:top_n]\n",
    "\n",
    "    # 创建图\n",
    "    G = nx.Graph()\n",
    "\n",
    "    try:\n",
    "        # 提取并添加节点到图中\n",
    "        mineral_locality_map = {}\n",
    "        for mineral in filtered_data:\n",
    "            mineral_id = mineral[\"id\"]\n",
    "            mineral_name = mineral[\"name\"]\n",
    "            localities = mineral[\"locality\"]\n",
    "            strunz_value = mineral.get(\"strunz10ed1\", 11)\n",
    "\n",
    "            # 确保 strunz_value 是有效的整数\n",
    "            try:\n",
    "                strunz_value = int(strunz_value)\n",
    "            except (ValueError, TypeError):\n",
    "                strunz_value = 11\n",
    "\n",
    "            color = color_map.get(int(strunz_value), 'black')\n",
    "\n",
    "            # 计算节点大小\n",
    "            node_size = 20 + math.log1p(len(localities)) * 10\n",
    "\n",
    "            # 添加矿物节点\n",
    "            G.add_node(mineral_id, label=mineral_name, color=color, size=node_size, font_size=20)\n",
    "\n",
    "            for locality in localities:\n",
    "                if locality not in mineral_locality_map:\n",
    "                    mineral_locality_map[locality] = set()\n",
    "                mineral_locality_map[locality].add(mineral_id)\n",
    "    except KeyError as e:\n",
    "        error_message = str(e) + \"\\n'expand':'locality' in geocollector data query is necessary for network visualization\"\n",
    "        return error_message\n",
    "\n",
    "    # 添加边\n",
    "    for locality, minerals in mineral_locality_map.items():\n",
    "        minerals = list(minerals)\n",
    "        for i in range(len(minerals)):\n",
    "            for j in range(i + 1, len(minerals)):\n",
    "                G.add_edge(minerals[i], minerals[j], color='grey')\n",
    "\n",
    "    # 创建 PyVis 网络\n",
    "    net = Network(notebook=True, height=\"750px\", width=\"100%\", cdn_resources='in_line')\n",
    "    net.repulsion(node_distance=400, central_gravity=0.1, spring_length=200, spring_strength=0.05, damping=0.09)\n",
    "    net.from_nx(G)\n",
    "\n",
    "    for node in net.nodes:\n",
    "        node[\"font\"] = {\"size\": node.get(\"size\", 20)}\n",
    "\n",
    "    # 保存网络图为 HTML 文件\n",
    "    output_file_path = \"minerals_network.html\"\n",
    "    net.save_graph(output_file_path)\n",
    "\n",
    "    # 插入图例到 HTML 内容\n",
    "    with open(output_file_path, 'r') as f:\n",
    "        html_content = f.read()\n",
    "    insertion_point = html_content.find('<body>') + len('<body>')\n",
    "    html_content_with_legend = html_content[:insertion_point] + legend_html + html_content[insertion_point:]\n",
    "\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        f.write(html_content_with_legend)\n",
    "\n",
    "    if display_method == 'html':\n",
    "        # 使用 HTML 显示（适用于支持的环境）\n",
    "        display(HTML(html_content_with_legend))\n",
    "    elif display_method == 'static':\n",
    "        # 使用 Matplotlib 创建静态图像\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        pos = nx.spring_layout(G)\n",
    "        nx.draw(G, pos, node_color=[G.nodes[node]['color'] for node in G.nodes()], \n",
    "                node_size=[G.nodes[node]['size'] * 10 for node in G.nodes()], \n",
    "                with_labels=True, font_size=8)\n",
    "        plt.title(\"Mineral Network\")\n",
    "        plt.savefig(\"minerals_network.png\")\n",
    "        display(Image(filename=\"minerals_network.png\"))\n",
    "    elif display_method == 'file':\n",
    "        # 仅保存文件，不尝试显示\n",
    "        print(f\"Network graph saved as {output_file_path}\")\n",
    "        print(\"Please open this file in a web browser to view the interactive network.\")\n",
    "    elif display_method == 'ipython':\n",
    "        # 使用 IPython 的 IFrame 显示\n",
    "        display(IFrame(src=output_file_path, width='100%', height='600px'))\n",
    "    elif display_method == 'new_window':\n",
    "        # 在新窗口中打开 HTML 文件\n",
    "        full_path = os.path.abspath(output_file_path)\n",
    "        webbrowser.open('file://' + full_path, new=2)\n",
    "        print(f\"Opening network graph in a new window: {full_path}\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid display_method. Choose 'html', 'static', 'file', 'ipython', or 'new_window'.\")\n",
    "\n",
    "    return \"Successfully created the network plot with the specified display method.\"\n",
    "\n",
    "# network_plot = StructuredTool.from_function(\n",
    "#     func=network_plot_function,\n",
    "#     name=\"NetworkPlot\",\n",
    "#     description=\"useful for plotting the mineral data in network diagram.\",\n",
    "#     args_schema=NetworkVizInput\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## network chain\n",
    "network_plotter_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "    You are a mineral data network plotter. Your team member will collect the dataset and give you the file path to work with.\n",
    "    All you need is the file path. \n",
    "    \n",
    "    The visualization tool is included.\n",
    "\n",
    "    If the file path to the dataset is present in the message, return with the file path. The visualization tool will process the file.\n",
    "    The file path can be in windows or macOS style.\n",
    "    If you didn't find the file path attached in the message, comment it in the scratch_pad field.\n",
    "\n",
    "    Pay attention to the error message if presents, revise your response accordingly.\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Messages : {messages} \\n\n",
    "    Errors: {errors}\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"messages\"],\n",
    ")\n",
    "\n",
    "# llm = OllamaFunctions(model=\"llama3:8b\", format='json')\n",
    "\n",
    "# collector_structured_llm = llm.with_structured_output(MindatCollectorInput)\n",
    "network_plotter_structured_llm = llm.with_structured_output(NetworkVizInput)\n",
    "network_chain = network_plotter_prompt | network_plotter_structured_llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### network test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error --no-raise-exception\n",
    "\n",
    "# messages = \"\"\"User\n",
    "# plot the histogram of the elements distribution of the ima-approved mineral species with hardness between 3-5, in Hexagonal crystal system, must have Neodymium, but without sulfur\"\"\"\n",
    "\n",
    "messages = \"please plot the mineral data for Norway, file_path: './mindat_data/geomaterial_data.json'\"\n",
    "errors = ''\n",
    "\n",
    "tolerance = 10\n",
    "for t in range(tolerance):\n",
    "    print(t)\n",
    "    try:\n",
    "        result = network_chain.invoke({\"messages\": messages, 'errors': errors})\n",
    "        break\n",
    "    except ValueError as e:\n",
    "        errors = str(e)\n",
    "        print(e)\n",
    "    \n",
    "print(result)\n",
    "print(type(result))\n",
    "print(result.file_path)\n",
    "print(type(result.file_path))\n",
    "network_plot_function(result.file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from IPython.display import display, IFrame\n",
    "\n",
    "class HeatmapVizInput(BaseModel):\n",
    "    file_path: str = Field(description=\"Should be a file path of json data for the tool to load and plot heatmap visualizations\")\n",
    "\n",
    "def heatmap_plot_function(file_path: str, visualization_selection='heatmap'):\n",
    "    # Initialize sums and counters\n",
    "    lat_sum = 0\n",
    "    lon_sum = 0\n",
    "    count = 0\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Sum up all latitudes and longitudes\n",
    "    for item in data['results']:\n",
    "        lat = item['latitude']\n",
    "        lon = item['longitude']\n",
    "        # Filter out the (0,0) coordinate and other potentially erroneous coordinates\n",
    "        if lat != 0.0 and lon != 0.0:\n",
    "            lat_sum += lat\n",
    "            lon_sum += lon\n",
    "            count += 1\n",
    "\n",
    "    # Calculate the average latitude and longitude (the centroid)\n",
    "    if count > 0:\n",
    "        center_lat = lat_sum / count\n",
    "        center_lon = lon_sum / count\n",
    "    else:\n",
    "        center_lat, center_lon = 38, 77  # Default to Washington, D.C. if no valid data points\n",
    "\n",
    "    # Create a map centered around the calculated centroid\n",
    "    map = folium.Map(location=[center_lat, center_lon], zoom_start=6)\n",
    "\n",
    "    if \"pop up\" == visualization_selection:\n",
    "        # Add markers for each location in the JSON data\n",
    "        for item in data['results']:\n",
    "            lat = item['latitude']\n",
    "            lon = item['longitude']\n",
    "            # Filter out the (0,0) coordinate\n",
    "            id = item.get('id')\n",
    "            # print(type(id))\n",
    "            txt = item.get('txt', 'No txt provided')  # Default if no description is provided\n",
    "            url = f'https://www.mindat.org/loc-{id}.html'\n",
    "            # popup_info = f\"<strong>{id}</strong><br>{txt}\"\n",
    "            popup_info = folium.Popup(f\"<div style='width:200px; font-size:16px;'><strong>ID:</strong> {id}<br><strong>Description:</strong> {txt}<br><strong>URL:</strong> <a href='{url}' target='_blank'>{url}</a></div>\",\n",
    "                                max_width=265)\n",
    "            if lat != 0.0 or lon != 0.0:\n",
    "                folium.Marker(\n",
    "                    location=[lat, lon],\n",
    "                    popup=popup_info,\n",
    "                    icon=folium.Icon(color='blue', icon='info-sign')\n",
    "                ).add_to(map)\n",
    "    elif \"heatmap\" == visualization_selection:\n",
    "        # Add markers for each location in the JSON data\n",
    "        for item in data['results']:\n",
    "            lat = item['latitude']\n",
    "            lon = item['longitude']\n",
    "            # Filter out the (0,0) coordinate\n",
    "\n",
    "        # Add a heat map layer to the map\n",
    "        heat_map_data = [\n",
    "            (item['latitude'], item['longitude']) for item in data['results']\n",
    "            if item['latitude'] != 0.0 and item['longitude'] != 0.0\n",
    "        ]\n",
    "\n",
    "        HeatMap(heat_map_data).add_to(map)\n",
    "    else:\n",
    "        raise ValueError(\"Please select a visualization approach!\")\n",
    "\n",
    "    # Save the map as HTML file\n",
    "    map_html = map._repr_html_()\n",
    "\n",
    "    # Display the map in the notebook\n",
    "    display(HTML(map_html))\n",
    "    return \"Successfully plotted the required diagram.\"\n",
    "\n",
    "\n",
    "heatmap_plot = StructuredTool.from_function(\n",
    "    func=heatmap_plot_function,\n",
    "    name=\"HeatmapPlot\",\n",
    "    description=\"useful for plotting the mineral locality data in heatmap diagram.\",\n",
    "    args_schema=HeatmapVizInput\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## heatmap chain\n",
    "heatmap_plotter_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "    You are a mineral locality heatmap plotter. Your team member will collect the dataset and give you the file path to work with.\n",
    "    All you need is the file path. \n",
    "    \n",
    "    The visualization tool is included.\n",
    "\n",
    "    If the file path to the dataset is present in the message, return with the file path. The visualization tool will process the file.\n",
    "    The file path can be in windows or macOS style.\n",
    "    If you didn't find the file path attached in the message, comment it in the scratch_pad field.\n",
    "\n",
    "    Pay attention to the error message if presents, revise your response accordingly.\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Messages : {messages} \\n\n",
    "    Errors: {errors}\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"messages\"],\n",
    ")\n",
    "\n",
    "# llm = OllamaFunctions(model=\"llama3:8b\", format='json')\n",
    "\n",
    "# collector_structured_llm = llm.with_structured_output(MindatCollectorInput)\n",
    "heatmap_plotter_structured_llm = llm.with_structured_output(HeatmapVizInput)\n",
    "heatmap_chain = heatmap_plotter_prompt | heatmap_plotter_structured_llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Heatmap test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error --no-raise-exception\n",
    "\n",
    "# messages = \"\"\"User\n",
    "# plot the histogram of the elements distribution of the ima-approved mineral species with hardness between 3-5, in Hexagonal crystal system, must have Neodymium, but without sulfur\"\"\"\n",
    "\n",
    "messages = \"please plot the mineral data for Norway, file_path: './mindat_data/locality_data.json'\"\n",
    "errors = ''\n",
    "\n",
    "tolerance = 10\n",
    "for t in range(tolerance):\n",
    "    print(t)\n",
    "    try:\n",
    "        result = heatmap_chain.invoke({\"messages\": messages, 'errors': errors})\n",
    "        break\n",
    "    except ValueError as e:\n",
    "        errors = str(e)\n",
    "        print(e)\n",
    "    \n",
    "print(result)\n",
    "print(type(result))\n",
    "print(result.file_path)\n",
    "print(type(result.file_path))\n",
    "heatmap_plot_function(result.file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import operator\n",
    "from typing import Annotated, Any, Dict, List, Optional, Sequence, TypedDict\n",
    "import functools\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "\n",
    "# The agent state is the input to each node in the graph\n",
    "# class AgentState(TypedDict):\n",
    "#     # The annotation tells the graph that new messages will always\n",
    "#     # be added to the current states\n",
    "#     messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "#     # The 'next' field indicates where to route to next\n",
    "#     next: str\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # The annotation tells the graph that new messages will always\n",
    "    # be added to the current states\n",
    "    messages: list\n",
    "    # The 'next' field indicates where to route to next\n",
    "    next: str\n",
    "    # Step count\n",
    "    step: int\n",
    "    # Plotter file history\n",
    "    plotter_file_history: str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "# from langchain_core.messages import BaseMessage, HumanMessage\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# def create_agent(llm: ChatOpenAI, tools: list, system_prompt: str):\n",
    "#     # Each worker node will be given a name and some tools.\n",
    "#     prompt = ChatPromptTemplate.from_messages(\n",
    "#         [\n",
    "#             (\n",
    "#                 \"system\",\n",
    "#                 system_prompt,\n",
    "#             ),\n",
    "#             MessagesPlaceholder(variable_name=\"messages\"),\n",
    "#             MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "#         ]\n",
    "#     )\n",
    "#     agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "#     executor = AgentExecutor(agent=agent, tools=tools)\n",
    "#     return executor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervisor Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervisor_node(state):\n",
    "    \"\"\"\n",
    "    Route user request to agent team members.\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "    print('====in supervisor node====')\n",
    "\n",
    "    # state message initialization\n",
    "    if not isinstance(state['messages'], list):\n",
    "        state['messages'] = [state['messages']]\n",
    "    # state step initialization\n",
    "    if 1 == len(state['messages']):\n",
    "        state['messages'][0] = \"User: \" + state['messages'][0]\n",
    "        state['step'] = 0\n",
    "        \n",
    "    # print out the message logs\n",
    "    messages = state['messages']\n",
    "    for msg in messages:\n",
    "        print(msg)\n",
    "\n",
    "    errors = ''\n",
    "\n",
    "    retry_tolerance = 10\n",
    "    \n",
    "    for retry_count in range(retry_tolerance):\n",
    "        print(f\"Retrying({retry_count}/{retry_tolerance})\", end='\\r')\n",
    "        try:\n",
    "            result = supervisor_chain.invoke({\n",
    "                \"members\": members, \n",
    "                \"messages\": str(messages), \n",
    "                \"options\":options, \n",
    "                \"errors\": errors,\n",
    "                \"supervisor_example1\": supervisor_example1,\n",
    "                \"supervisor_example2\": supervisor_example2,\n",
    "                \"supervisor_example3\": supervisor_example3,\n",
    "            })\n",
    "            # result_next = result.tool_calls[0]['args']['next']\n",
    "            # if result_next in options:\n",
    "            #     break\n",
    "            # else:\n",
    "            #     errors = f\"{result_next} not in {options}, please try again.\"\n",
    "            # print(\"supervisor output: \", result.tool_calls[0]['args'])\n",
    "            \n",
    "            print(\"supervisor result: \", result)\n",
    "            # raw_result = result.tool_calls[0]['args']\n",
    "            raw_result = result.next\n",
    "\n",
    "            result_next = next_parser(raw_result)\n",
    "            # result_next = str(result.tool_calls[0]['args']['next'])\n",
    "            if result_next:\n",
    "                break\n",
    "            else:\n",
    "                jargon_errors = f\"{raw_result} not in {options}, please try again.\"\n",
    "                errors = jargon_errors\n",
    "                print(\"Jargon errors: \", jargon_errors)\n",
    "\n",
    "        except (ValueError, KeyError, IndexError, AttributeError) as e:\n",
    "            error_type = type(e).__name__\n",
    "            # print(\"!!error type: \", error_type)\n",
    "            if \"ValueError\" == str(error_type) or \"AttributeError\" == str(error_type):\n",
    "                print(str(e))\n",
    "                try:\n",
    "                    result_dict = parse_required_keys_from_llm_response(str(e))\n",
    "                    result_next = next_parser(result_dict)\n",
    "                    if result_next:\n",
    "                        break\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "            # if \"AttributeError\" == str(error_type): \n",
    "            #     result_next = next_parser(str(result))\n",
    "            #     if result_next:\n",
    "            #         break\n",
    "\n",
    "            errors = f\"{error_type}: {str(e)}\"\n",
    "            print(errors)\n",
    "    \n",
    "    \n",
    "    updated_message = state['messages'] + [\"Step {step}, Supervisor node: {result_next}\".format(step=state['step'], result_next=result_next)]\n",
    "\n",
    "    return {\n",
    "        \"messages\": updated_message,\n",
    "        \"next\": result_next,\n",
    "        \"step\": state['step'] + 1\n",
    "    }\n",
    "\n",
    "def next_parser(JARGON: dict):\n",
    "    # options = ['COLLECTOR', 'HISTOGRAM_PLOTTER', 'FINISH']\n",
    "    jargon_str = str(JARGON).upper()\n",
    "    for option in options:\n",
    "        if option in jargon_str:\n",
    "            return option\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def parse_required_keys_from_llm_response(llm_response):\n",
    "    # 定义所需的键\n",
    "    required_keys = [\"next\", \"supervisor_reason\"]\n",
    "    extracted_data = {}\n",
    "\n",
    "    # 使用正则表达式匹配所有键值对，匹配键和值为双引号或单引号包裹的内容\n",
    "    for key in required_keys:\n",
    "        pattern = rf'[\"]({key})[\"](?:\\s*:\\s*)[\"](.*?)[\"]'\n",
    "        match = re.search(pattern, llm_response, re.DOTALL)\n",
    "        if match:\n",
    "            extracted_data[match.group(1)] = match.group(2)\n",
    "\n",
    "    # 检查是否所有键都被提取\n",
    "    if len(extracted_data) == len(required_keys):\n",
    "        return json.loads(json.dumps(extracted_data))\n",
    "    else:\n",
    "        missing_keys = set(required_keys) - set(extracted_data.keys())\n",
    "        raise ValueError(f\"Missing keys in LLM response: {missing_keys}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geo Collector Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_collector_node(state):\n",
    "    print(\"====in geo collector node====\")\n",
    "    messages = state['messages']\n",
    "    errors = ''\n",
    "\n",
    "    retry_tolerance = 10\n",
    "    for retry_count in range(retry_tolerance):\n",
    "        print(f\"Retrying({retry_count}/{retry_tolerance})\", end='\\r')\n",
    "        try:\n",
    "            result = geo_collector_chain.invoke({\n",
    "                    \"messages\": str(messages), \n",
    "                    \"errors\": errors, \n",
    "                    'example_query1': example_query1,\n",
    "                    'example_query2': example_query2,\n",
    "                    'example_query3': example_query3,\n",
    "                })\n",
    "            result_querydict = result.tool_calls[0]['args']\n",
    "            collector_result = geomaterial_collector_function(result_querydict)\n",
    "            if collector_result:\n",
    "                break\n",
    "            else:\n",
    "                errors = \"The query has no result, please read the user request and try again.\"\n",
    "\n",
    "        except (ValueError, IndexError, KeyError) as e:\n",
    "            error_type = type(e).__name__\n",
    "            errors = f\"{error_type}: {str(e)}\"\n",
    "            print(errors)\n",
    "\n",
    "    # print(result)\n",
    "    openmindat_results = str(collector_result)\n",
    "\n",
    "    updated_messages = state['messages'] + [\"Step {step}, Collector node: {openmindat_results}\".format(step=state['step'], openmindat_results=openmindat_results)]\n",
    "    return {\n",
    "                \"messages\": updated_messages,\n",
    "                \"step\": state['step'] + 1\n",
    "            }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loc Collector Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc_collector_node(state):\n",
    "    print(\"====in loc collector node====\")\n",
    "    messages = state['messages']\n",
    "    errors = ''\n",
    "\n",
    "    retry_tolerance = 10\n",
    "    for retry_count in range(retry_tolerance):\n",
    "        print(f\"Retrying({retry_count}/{retry_tolerance})\", end='\\r')\n",
    "        try:\n",
    "            result = loc_collector_chain.invoke({\n",
    "                    \"messages\": str(messages), \n",
    "                    \"errors\": errors, \n",
    "                    'example_loc_query1': example_loc_query1,\n",
    "                    'example_loc_query2': example_loc_query2,\n",
    "                    'example_loc_query3': example_loc_query3,\n",
    "                })\n",
    "            result_querydict = result.tool_calls[0]['args']\n",
    "            collector_result = locality_collector_function(result_querydict)\n",
    "            if collector_result:\n",
    "                break\n",
    "            else:\n",
    "                errors = \"The query has no result, please read the user request and try again.\"\n",
    "\n",
    "        except (ValueError, IndexError, KeyError) as e:\n",
    "            error_type = type(e).__name__\n",
    "            errors = f\"{error_type}: {str(e)}\"\n",
    "            print(errors)\n",
    "\n",
    "    # print(result)\n",
    "    openmindat_results = str(collector_result)\n",
    "\n",
    "    updated_messages = state['messages'] + [\"Step {step}, Collector node: {openmindat_results}\".format(step=state['step'], openmindat_results=openmindat_results)]\n",
    "    return {\n",
    "                \"messages\": updated_messages,\n",
    "                \"step\": state['step'] + 1\n",
    "            }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error --no-raise-exception\n",
    "def histogram_plotter_node(state):\n",
    "    print(\"====in histogram plotter node====\")\n",
    "    messages = state['messages']\n",
    "    errors = ''\n",
    "\n",
    "    retry_tolerance = 10\n",
    "    for retry_count in range(retry_tolerance):\n",
    "        print(f\"Retrying({retry_count}/{retry_tolerance})\", end='\\r')\n",
    "        try:\n",
    "            result = histogram_chain.invoke({\"messages\": str(messages), \"errors\": errors})\n",
    "            result_path = result.tool_calls[0]['args']['file_path']\n",
    "            break\n",
    "\n",
    "        except (ValueError, KeyError) as e:\n",
    "            error_type = type(e).__name__\n",
    "            errors = f\"{error_type}: {str(e)}\"\n",
    "            print(errors)\n",
    "\n",
    "    path_obj = Path(result_path)\n",
    "\n",
    "    updated_plot_history = ''\n",
    "\n",
    "    if str(path_obj) == state['plotter_file_history']:\n",
    "        redundant_message = \"I have already finished the requested histogram plotting. If you have another new request, please let me know.\"\n",
    "        updated_messages = state['messages'] + [\"Step {step}, Histogram plotter node: {redundant_message}\".format(step=state['step'], redundant_message=redundant_message)]\n",
    "        updated_plot_history = state['plotter_file_history']\n",
    "        \n",
    "    elif path_obj.exists() and Path('.') != path_obj:\n",
    "        histogram_plot_function(path_obj)\n",
    "        successful_message = \"I have successfully finished the histogram request.\"\n",
    "        updated_messages = state['messages'] + [\"Step {step}, Histogram plotter node: {successful_message}\".format(step=state['step'], successful_message=successful_message)]\n",
    "        updated_plot_history = str(path_obj)\n",
    "\n",
    "        print(\"!!!printing test: \\nstr(path_obj):\", str(path_obj), \"state['plotter_file_history']:\", state['plotter_file_history'])\n",
    "    else:\n",
    "        file_missing_message = \"The file path was not a valid path, please collect the dataset first.\"\n",
    "        updated_messages = state['messages'] + [\"Step {step}, Histogram plotter node: {file_missing_message}\".format(step=state['step'], file_missing_message=file_missing_message)]\n",
    "\n",
    "    return {\n",
    "                \"messages\": updated_messages,\n",
    "                \"step\": state['step'] + 1,\n",
    "                \"plotter_file_history\": updated_plot_history\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Plotter Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_plotter_node(state):\n",
    "    print(\"====in network plotter node====\")\n",
    "    messages = state['messages']\n",
    "    errors = ''\n",
    "\n",
    "    retry_tolerance = 10\n",
    "    for retry_count in range(retry_tolerance):\n",
    "        print(f\"Retrying({retry_count}/{retry_tolerance})\", end='\\r')\n",
    "        try:\n",
    "            result = network_chain.invoke({\"messages\": str(messages), \"errors\": errors})\n",
    "            # result_path = result.tool_calls[0]['args']['file_path']\n",
    "            result_path = result.file_path\n",
    "            break\n",
    "\n",
    "        except (ValueError, KeyError, AttributeError) as e:\n",
    "            error_type = type(e).__name__\n",
    "            errors = f\"{error_type}: {str(e)}\"\n",
    "            print(errors)\n",
    "\n",
    "    path_obj = Path(result_path)\n",
    "\n",
    "    updated_plot_history = ''\n",
    "\n",
    "    if str(path_obj) == state.get('plotter_file_history'):\n",
    "        redundant_message = \"I have already finished the requested Network plotting. If you have another new request, please let me know.\"\n",
    "        updated_messages = state['messages'] + [\"Step {step}, Network plotter node: {redundant_message}\".format(step=state['step'], redundant_message=redundant_message)]\n",
    "        updated_plot_history = state['plotter_file_history']\n",
    "        \n",
    "    elif path_obj.exists() and Path('.') != path_obj:\n",
    "        network_plot_function(path_obj)\n",
    "        successful_message = \"I have successfully finished the Network request.\"\n",
    "        updated_messages = state['messages'] + [\"Step {step}, Network plotter node: {successful_message}\".format(step=state['step'], successful_message=successful_message)]\n",
    "        updated_plot_history = str(path_obj)\n",
    "\n",
    "        # print(\"!!!printing test: \\nstr(path_obj):\", str(path_obj), \"state['plotter_file_history']:\", state.get('plotter_file_history'))\n",
    "    else:\n",
    "        file_missing_message = \"The file path was not a valid path, please collect the dataset first.\"\n",
    "        updated_messages = state['messages'] + [\"Step {step}, Network plotter node: {file_missing_message}\".format(step=state['step'], file_missing_message=file_missing_message)]\n",
    "\n",
    "    return {\n",
    "                \"messages\": updated_messages,\n",
    "                \"step\": state['step'] + 1,\n",
    "                \"plotter_file_history\": updated_plot_history\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap Plotter Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_plotter_node(state):\n",
    "    print(\"====in heatmap plotter node====\")\n",
    "    messages = state['messages']\n",
    "    errors = ''\n",
    "\n",
    "    retry_tolerance = 10\n",
    "    for retry_count in range(retry_tolerance):\n",
    "        print(f\"Retrying({retry_count}/{retry_tolerance})\", end='\\r')\n",
    "        try:\n",
    "            result = heatmap_chain.invoke({\"messages\": str(messages), \"errors\": errors})\n",
    "            # result_path = result.tool_calls[0]['args']['file_path']\n",
    "            result_path = result.file_path\n",
    "            break\n",
    "\n",
    "        except (ValueError, KeyError, AttributeError) as e:\n",
    "            error_type = type(e).__name__\n",
    "            errors = f\"{error_type}: {str(e)}\"\n",
    "            print(errors)\n",
    "\n",
    "    path_obj = Path(result_path)\n",
    "\n",
    "    updated_plot_history = ''\n",
    "\n",
    "    if str(path_obj) == state.get('plotter_file_history'):\n",
    "        redundant_message = \"I have already finished the requested heatmap plotting. If you have another new request, please let me know.\"\n",
    "        updated_messages = state['messages'] + [\"Step {step}, heatmap plotter node: {redundant_message}\".format(step=state['step'], redundant_message=redundant_message)]\n",
    "        updated_plot_history = state['plotter_file_history']\n",
    "        \n",
    "    elif path_obj.exists() and Path('.') != path_obj:\n",
    "        heatmap_plot_function(path_obj)\n",
    "        successful_message = \"I have successfully finished the heatmap request.\"\n",
    "        updated_messages = state['messages'] + [\"Step {step}, heatmap plotter node: {successful_message}\".format(step=state['step'], successful_message=successful_message)]\n",
    "        updated_plot_history = str(path_obj)\n",
    "\n",
    "        # print(\"!!!printing test: \\nstr(path_obj):\", str(path_obj), \"state['plotter_file_history']:\", state.get('plotter_file_history'))\n",
    "    else:\n",
    "        file_missing_message = \"The file path was not a valid path, please collect the dataset first.\"\n",
    "        updated_messages = state['messages'] + [\"Step {step}, heatmap plotter node: {file_missing_message}\".format(step=state['step'], file_missing_message=file_missing_message)]\n",
    "\n",
    "    return {\n",
    "                \"messages\": updated_messages,\n",
    "                \"step\": state['step'] + 1,\n",
    "                \"plotter_file_history\": updated_plot_history\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_to_agents(state):\n",
    "    \"\"\"\n",
    "    Route user request to agent team members.\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print('====route to team members====')\n",
    "    next = state['next']\n",
    "\n",
    "    # state['messages'] = list(state['messages']) + [\"Supervisor node: 'next': {next}\"]\n",
    "    \n",
    "    return next.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x13923cd70>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"supervisor_node\", supervisor_node)\n",
    "workflow.add_node(\"geo_collector_node\", geo_collector_node)\n",
    "workflow.add_node(\"loc_collector_node\", loc_collector_node)\n",
    "# workflow.add_node(\"histogram_plotter_node\", histogram_plotter_node)\n",
    "workflow.add_node(\"network_plotter_node\", network_plotter_node)\n",
    "workflow.add_node(\"heatmap_plotter_node\", heatmap_plotter_node)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x13923cd70>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.set_entry_point(\"supervisor_node\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"supervisor_node\",\n",
    "    route_to_agents,\n",
    "    {\n",
    "        \"GEOMATERIAL_COLLECTOR\": \"geo_collector_node\",\n",
    "        \"LOCALITY_COLLECTOR\": \"loc_collector_node\",\n",
    "        # \"HISTOGRAM_PLOTTER\": \"histogram_plotter_node\",\n",
    "        \"NETWORK_PLOTTER\": \"network_plotter_node\",\n",
    "        \"HEATMAP_PLOTTER\": \"heatmap_plotter_node\",\n",
    "        \"FINISH\": END,\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"geo_collector_node\", \"supervisor_node\")\n",
    "workflow.add_edge(\"loc_collector_node\", \"supervisor_node\")\n",
    "workflow.add_edge(\"network_plotter_node\", \"supervisor_node\")\n",
    "workflow.add_edge(\"heatmap_plotter_node\", \"supervisor_node\")\n",
    "# workflow.add_edge(\"histogram_plotter_node\", \"supervisor_node\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "compiled_graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(compiled_graph.get_graph().draw_mermaid_png()))\n",
    "except:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====in supervisor node====\n",
      "User: i want you to plot network visualization for carbon and iron mineral but without Magnesium\n",
      "supervisor result:  next='GEOMATERIAL_COLLECTOR' supervisor_reason='The user want to plot network visualization, need to collect mineral dataset first.'\n",
      "====route to team members====\n",
      "'Finished running: supervisor_node'\n",
      "====in geo collector node====\n",
      "====in geomaterial collecting function====\n",
      "{'elements_exc': 'Mg', 'elements_inc': 'C,Fe', 'ima': True}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c32364ddb51421787e514eeef25c52b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching data:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 28 entries to /Users/blc/Documents/Manuscripts/2409_OpenLLMWorkflow/Code/jupyter/Evaluation/llama3.1/mindat_data/geomaterial_data.json\n",
      "'Finished running: geo_collector_node'\n",
      "====in supervisor node====\n",
      "User: i want you to plot network visualization for carbon and iron mineral but without Magnesium\n",
      "Step 0, Supervisor node: GEOMATERIAL_COLLECTOR\n",
      "Step 1, Collector node: The Geomaterial Collector node has successfully saved the dataset to /Users/blc/Documents/Manuscripts/2409_OpenLLMWorkflow/Code/jupyter/Evaluation/llama3.1/mindat_data/geomaterial_data.json\n",
      "supervisor result:  next='NETWORK_PLOTTER' supervisor_reason='The dataset is collected, and the user wants to see the network visualization'\n",
      "====route to team members====\n",
      "'Finished running: supervisor_node'\n",
      "====in network plotter node====\n",
      "'Finished running: network_plotter_node'\n",
      "====in supervisor node====\n",
      "User: i want you to plot network visualization for carbon and iron mineral but without Magnesium\n",
      "Step 0, Supervisor node: GEOMATERIAL_COLLECTOR\n",
      "Step 1, Collector node: The Geomaterial Collector node has successfully saved the dataset to /Users/blc/Documents/Manuscripts/2409_OpenLLMWorkflow/Code/jupyter/Evaluation/llama3.1/mindat_data/geomaterial_data.json\n",
      "Step 2, Supervisor node: NETWORK_PLOTTER\n",
      "Step 3, Network plotter node: I have successfully finished the Network request.\n",
      "supervisor result:  next='HEATMAP_PLOTTER' supervisor_reason='The network plotter has finished the user request, now we need to see the heatmap for carbon and iron mineral without Magnesium'\n",
      "====route to team members====\n",
      "'Finished running: supervisor_node'\n",
      "====in heatmap plotter node====\n",
      "'Finished running: heatmap_plotter_node'\n",
      "====in supervisor node====\n",
      "User: i want you to plot network visualization for carbon and iron mineral but without Magnesium\n",
      "Step 0, Supervisor node: GEOMATERIAL_COLLECTOR\n",
      "Step 1, Collector node: The Geomaterial Collector node has successfully saved the dataset to /Users/blc/Documents/Manuscripts/2409_OpenLLMWorkflow/Code/jupyter/Evaluation/llama3.1/mindat_data/geomaterial_data.json\n",
      "Step 2, Supervisor node: NETWORK_PLOTTER\n",
      "Step 3, Network plotter node: I have successfully finished the Network request.\n",
      "Step 4, Supervisor node: HEATMAP_PLOTTER\n",
      "Step 5, heatmap plotter node: I have already finished the requested heatmap plotting. If you have another new request, please let me know.\n",
      "Retrying(0/10)\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m message_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: inputs}\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# inputs = {\"messages\": \"this is a testflight, please respond with Finish\"}\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompiled_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_dict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpprint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFinished running: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mkey\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1273\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1262\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1266\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1268\u001b[0m         input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[1;32m   1269\u001b[0m         interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before_,\n\u001b[1;32m   1270\u001b[0m         interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after_,\n\u001b[1;32m   1271\u001b[0m         manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   1272\u001b[0m     ):\n\u001b[0;32m-> 1273\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[1;32m   1280\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/langgraph/pregel/runner.py:56\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m     54\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/langgraph/pregel/retry.py:29\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     27\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/langgraph/utils/runnable.py:410\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/langgraph/utils/runnable.py:184\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 184\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[26], line 31\u001b[0m, in \u001b[0;36msupervisor_node\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretry_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretry_tolerance\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43msupervisor_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmembers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmembers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43merrors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msupervisor_example1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msupervisor_example1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msupervisor_example2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msupervisor_example2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msupervisor_example3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msupervisor_example3\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# result_next = result.tool_calls[0]['args']['next']\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# if result_next in options:\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m#     break\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m#     errors = f\"{result_next} not in {options}, please try again.\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# print(\"supervisor output: \", result.tool_calls[0]['args'])\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupervisor result: \u001b[39m\u001b[38;5;124m\"\u001b[39m, result)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/langchain_core/runnables/base.py:3024\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3023\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3024\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3025\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3026\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/langchain_core/runnables/base.py:5354\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5350\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   5351\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5352\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5353\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5355\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5356\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5357\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5358\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    285\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 286\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    647\u001b[0m ]\n\u001b[1;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 633\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m         )\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/langchain_ollama/chat_models.py:644\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    639\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    643\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[0;32m--> 644\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m final_chunk\u001b[38;5;241m.\u001b[39mgeneration_info\n\u001b[1;32m    648\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[1;32m    649\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(\n\u001b[1;32m    650\u001b[0m             content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    654\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[1;32m    655\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/langchain_ollama/chat_models.py:545\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[0;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    538\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    543\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[1;32m    544\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mChatGenerationChunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAIMessageChunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/langchain_ollama/chat_models.py:517\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptions\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m stop\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m--> 517\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mollama_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOptions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkeep_alive\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mchat(\n\u001b[1;32m    528\u001b[0m         model\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    529\u001b[0m         messages\u001b[38;5;241m=\u001b[39mollama_messages,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    534\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/ollama/_client.py:236\u001b[0m, in \u001b[0;36mClient.chat\u001b[0;34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[0m\n\u001b[1;32m    233\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;241m:=\u001b[39m message\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    234\u001b[0m     message[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [_encode_image(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[0;32m--> 236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m  \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkeep_alive\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m  \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m  \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/ollama/_client.py:99\u001b[0m, in \u001b[0;36mClient._request_stream\u001b[0;34m(self, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request_stream\u001b[39m(\n\u001b[1;32m     94\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     95\u001b[0m   \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m     96\u001b[0m   stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     97\u001b[0m   \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     98\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Mapping[\u001b[38;5;28mstr\u001b[39m, Any], Iterator[Mapping[\u001b[38;5;28mstr\u001b[39m, Any]]]:\n\u001b[0;32m---> 99\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/ollama/_client.py:70\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, method, url, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m httpx\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[0;32m---> 70\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/httpx/_client.py:837\u001b[0m, in \u001b[0;36mClient.request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    822\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m    824\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[1;32m    825\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    826\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    835\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[1;32m    836\u001b[0m )\n\u001b[0;32m--> 837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/httpx/_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    924\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 926\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/httpx/_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    951\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 954\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/httpx/_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    989\u001b[0m     hook(request)\n\u001b[0;32m--> 991\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/httpx/_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1023\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1024\u001b[0m     )\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1027\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1031\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/httpx/_transports/default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    223\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    224\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    225\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    234\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 236\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    241\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    242\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    243\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    244\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    245\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/httpcore/_sync/http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/httpcore/_sync/http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "inputs = \"i want you to plot network visualization for carbon and iron mineral but without Magnesium\"\n",
    "# inputs = \"I want to know the spatial distribution of ophiolite.\"\n",
    "# inputs = \"i want you to plot histogram visualization for carbon and oxygen mineral but without Magnesium\"\n",
    "# inputs = \"i want to order a burger\"\n",
    "# inputs = \"trsghreiobje\"\n",
    "# run the agent\n",
    "message_dict = {\"messages\": inputs}\n",
    "\n",
    "# inputs = {\"messages\": \"this is a testflight, please respond with Finish\"}\n",
    "for output in compiled_graph.stream(message_dict):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====in supervisor node====\n",
      "User: I want to plot the locality data for brazil and thailand in heatmap\n",
      "supervisor result:  next='LOCALITY_COLLECTOR' supervisor_reason='I need to collect the locality dataset for plotting heatmap'\n",
      "====route to team members====\n",
      "'Finished running: supervisor_node'\n",
      "====in loc collector node====\n",
      "====in locality collecting function====\n",
      "{'country': 'Brazil'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd9e851f22b4430a1c8731e0ac2d87f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching data: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m message_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: inputs}\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# inputs = {\"messages\": \"this is a testflight, please respond with Finish\"}\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompiled_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_dict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpprint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFinished running: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mkey\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1273\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1262\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1266\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1268\u001b[0m         input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[1;32m   1269\u001b[0m         interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before_,\n\u001b[1;32m   1270\u001b[0m         interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after_,\n\u001b[1;32m   1271\u001b[0m         manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   1272\u001b[0m     ):\n\u001b[0;32m-> 1273\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[1;32m   1280\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/langgraph/pregel/runner.py:56\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m     54\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/langgraph/pregel/retry.py:29\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     27\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/langgraph/utils/runnable.py:410\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/langgraph/utils/runnable.py:184\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 184\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[34], line 18\u001b[0m, in \u001b[0;36mloc_collector_node\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     10\u001b[0m result \u001b[38;5;241m=\u001b[39m loc_collector_chain\u001b[38;5;241m.\u001b[39minvoke({\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(messages), \n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m\"\u001b[39m: errors, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexample_loc_query3\u001b[39m\u001b[38;5;124m'\u001b[39m: example_loc_query3,\n\u001b[1;32m     16\u001b[0m     })\n\u001b[1;32m     17\u001b[0m result_querydict \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mtool_calls[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 18\u001b[0m collector_result \u001b[38;5;241m=\u001b[39m \u001b[43mlocality_collector_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_querydict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collector_result:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 26\u001b[0m, in \u001b[0;36mlocality_collector_function\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     23\u001b[0m saving_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./mindat_data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mresolve()\n\u001b[1;32m     24\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocality_data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mlr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveto\u001b[49m\u001b[43m(\u001b[49m\u001b[43msaving_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m file_path \u001b[38;5;241m=\u001b[39m Path(saving_path, file_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m locality_checker(file_path):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/openmindat/localities.py:402\u001b[0m, in \u001b[0;36mLocalitiesRetriever.saveto\u001b[0;34m(self, OUTDIR, FILE_NAME)\u001b[0m\n\u001b[1;32m    399\u001b[0m verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose_flag\n\u001b[1;32m    401\u001b[0m ma \u001b[38;5;241m=\u001b[39m mindat_api\u001b[38;5;241m.\u001b[39mMindatApi()\n\u001b[0;32m--> 402\u001b[0m \u001b[43mma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_mindat_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_point\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;66;03m# reset the query parameters in case the user wants to make another query\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_params()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/openmindat/mindat_api.py:312\u001b[0m, in \u001b[0;36mMindatApi.download_mindat_json\u001b[0;34m(self, QUERY_DICT, END_POINT, OUTDIR, FILE_NAME, VERBOSE)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;124;03m    get all items in a list\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03m    Since this API has a limit of 1000 items per page,\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    we need to loop through all pages and save them to a single json file\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# get the json data\u001b[39;00m\n\u001b[0;32m--> 312\u001b[0m json_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mindat_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQUERY_DICT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEND_POINT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVERBOSE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# The default output name is same as the endpoint\u001b[39;00m\n\u001b[1;32m    315\u001b[0m file_name \u001b[38;5;241m=\u001b[39m FILE_NAME \u001b[38;5;28;01mif\u001b[39;00m FILE_NAME \u001b[38;5;28;01melse\u001b[39;00m END_POINT   \n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/openmindat/mindat_api.py:273\u001b[0m, in \u001b[0;36mMindatApi.get_mindat_json\u001b[0;34m(self, PARAM_DICT, END_POINT, VERBOSE)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m server_fail_count \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m):\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 273\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVERBOSE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m VERBOSE \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    275\u001b[0m             pbar\u001b[38;5;241m.\u001b[39mset_postfix()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/openmindat/mindat_api.py:192\u001b[0m, in \u001b[0;36mMindatApi.get_results\u001b[0;34m(self, URL, json_data, pbar, VERBOSE)\u001b[0m\n\u001b[1;32m    189\u001b[0m url \u001b[38;5;241m=\u001b[39m URL        \n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_headers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m     new_results \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    194\u001b[0m     json_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_results\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/site-packages/urllib3/connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/http/client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1428\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/ssl.py:1251\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1249\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ollama2/lib/python3.12/ssl.py:1103\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# inputs = \"i want to plot the locality data for Norway in heatmap\"\n",
    "# inputs = 'can you plot the locality data for north Europe in heatmap.'\n",
    "inputs = 'I want to plot the locality data for brazil and thailand in heatmap'\n",
    "# inputs = \"i want you to plot histogram visualization for carbon and oxygen mineral but without Magnesium\"\n",
    "# inputs = \"i want to order a burger\"\n",
    "# inputs = \"trsghreiobje\"\n",
    "# run the agent\n",
    "message_dict = {\"messages\": inputs}\n",
    "\n",
    "# inputs = {\"messages\": \"this is a testflight, please respond with Finish\"}\n",
    "for output in compiled_graph.stream(message_dict):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervisor Chain Output: next='GEOMATERIAL_COLLECTOR' supervisor_reason='I need to collect the mineral dataset to execute the user request'\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error --no-raise-exception\n",
    "\n",
    "messages = \"i want you to plot network visualization for carbon and iron mineral but without Magnesium\"\n",
    "# inputs = \"I want to know the spatial distribution of ophiolite.\"\n",
    "# inputs = \"i want you to plot histogram visualization for carbon and oxygen mineral but without Magnesium\"\n",
    "# inputs = \"i want to order a burger\"\n",
    "# inputs = \"trsghreiobje\"\n",
    "# run the agent\n",
    "\n",
    "result = supervisor_chain.invoke({\n",
    "                \"members\": members, \n",
    "                \"messages\": str(messages), \n",
    "                \"options\":options, \n",
    "                \"errors\": errors,\n",
    "                \"supervisor_example1\": supervisor_example1,\n",
    "                \"supervisor_example2\": supervisor_example2,\n",
    "                \"supervisor_example3\": supervisor_example3,\n",
    "            })\n",
    "\n",
    "\n",
    "print(\"Supervisor Chain Output:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "343ce6937d7f4b5a8c7c3b92c6161444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Tasks:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task processing completed and saved to evaluated_tasks.json and statistics.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Initialize statistics dictionary\n",
    "statistics = {\n",
    "    \"FP\": [],  # False Positive\n",
    "    \"FN\": [],  # False Negative  \n",
    "    \"TP\": [],  # True Positive\n",
    "    \"TN\": []   # True Negative\n",
    "}\n",
    "\n",
    "# Read JSON file\n",
    "with open('test_union_1104.json', 'r') as f:\n",
    "    tasks = json.load(f)\n",
    "\n",
    "# Iterate through each task and call LLM Chain\n",
    "for task in tqdm(tasks, desc=\"Processing Tasks\", total=len(tasks)):\n",
    "    # Prepare input data\n",
    "\n",
    "    messages = task[\"input\"]\n",
    "    input_data = {\n",
    "                \"members\": members, \n",
    "                \"messages\": str(messages), \n",
    "                \"options\":options, \n",
    "                \"errors\": errors,\n",
    "                \"supervisor_example1\": supervisor_example1,\n",
    "                \"supervisor_example2\": supervisor_example2,\n",
    "                \"supervisor_example3\": supervisor_example3,\n",
    "            }\n",
    "\n",
    "   # Call chain and capture output\n",
    "    try:\n",
    "        result = supervisor_chain.invoke(input_data)\n",
    "        evaluation_output = result  # Assume result is in dictionary format\n",
    "\n",
    "        # Add evaluation_output to task dictionary\n",
    "        task[\"evaluation_output\"] = str(evaluation_output)\n",
    "\n",
    "        # Get next values for comparison\n",
    "        output_next = task[\"output\"].get(\"next\")\n",
    "        eval_next = next_parser(evaluation_output)\n",
    "\n",
    "       # Classification statistics and add evaluation tag\n",
    "        if eval_next == output_next:  # Predicted value matches actual value\n",
    "            if eval_next == \"FINISH\":  # Matched finish state\n",
    "                statistics[\"TN\"].append(task[\"id\"]) \n",
    "                task[\"evaluation_tag\"] = \"TN\"  # True Negative - Correctly predicted finish\n",
    "            else:  # Matched other states (continue to next step)\n",
    "                statistics[\"TP\"].append(task[\"id\"])\n",
    "                task[\"evaluation_tag\"] = \"TP\"  # True Positive - Correctly predicted next step\n",
    "        else:  # Predicted value does not match actual value\n",
    "            if eval_next == \"FINISH\":  # Model predicted finish but should continue\n",
    "                statistics[\"FN\"].append(task[\"id\"])\n",
    "                task[\"evaluation_tag\"] = \"FN\"  # False Negative - Incorrectly predicted finish\n",
    "            else:  # Model predicted wrong next step\n",
    "                statistics[\"FP\"].append(task[\"id\"])\n",
    "                task[\"evaluation_tag\"] = \"FP\"  # False Positive - Predicted wrong continue state\n",
    "\n",
    "        # Optional: Add detailed evaluation information\n",
    "        task[\"evaluation_details\"] = {\n",
    "            \"expected_next\": output_next,\n",
    "            \"predicted_next\": eval_next,\n",
    "            \"is_match\": eval_next == output_next,\n",
    "            \"tag\": task[\"evaluation_tag\"]\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing task {task['id']}: {str(e)}\")\n",
    "        task[\"evaluation_tag\"] = \"ERROR\"\n",
    "        task[\"evaluation_details\"] = {\"error\": str(e)}\n",
    "\n",
    "\n",
    "def next_parser(JARGON: str):\n",
    "    jargon_str = str(JARGON).upper()\n",
    "\n",
    "    found_positions = {}\n",
    "    # options = ['FINISH', 'GEOMATERIAL_COLLECTOR', 'LOCALITY_COLLECTOR', 'NETWORK_PLOTTER', 'HEATMAP_PLOTTER']\n",
    "\n",
    "    for option in options:\n",
    "        if option in jargon_str:\n",
    "            found_positions[option] = jargon_str.index(option)\n",
    "    \n",
    "    # Return with the first value in the output\n",
    "    if found_positions:\n",
    "        return min(found_positions.items(), key=lambda x: x[1])[0]\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Save processed tasks and statistics to new JSON files  \n",
    "with open('evaluated_tasks.json', 'w') as f:\n",
    "    json.dump(tasks, f, indent=2)\n",
    "\n",
    "# Save statistics to separate statistics file\n",
    "with open('statistics.json', 'w') as f:\n",
    "    json.dump(statistics, f, indent=2)\n",
    "\n",
    "print(\"Task processing completed and saved to evaluated_tasks.json and statistics.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP (False Positive): 63\n",
      "FN (False Negative): 1\n",
      "TP (True Positive): 73\n",
      "TN (True Negative): 15\n",
      "\n",
      "Rate:\n",
      "FP Ratio: 41.45%\n",
      "FN Ratio: 0.66%\n",
      "TP Ratio: 48.03%\n",
      "TN Ratio: 9.87%\n",
      "\n",
      "Precision: 0.5368\n",
      "Recall: 0.9865\n",
      "F1 Score: 0.6952\n",
      "Accuracy: 0.5789\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('statistics.json', 'r') as f:\n",
    "    statistics = json.load(f)\n",
    "\n",
    "FP = len(statistics.get(\"FP\", []))\n",
    "FN = len(statistics.get(\"FN\", []))\n",
    "TP = len(statistics.get(\"TP\", []))\n",
    "TN = len(statistics.get(\"TN\", []))\n",
    "\n",
    "print(f\"FP (False Positive): {FP}\")\n",
    "print(f\"FN (False Negative): {FN}\")\n",
    "print(f\"TP (True Positive): {TP}\")\n",
    "print(f\"TN (True Negative): {TN}\")\n",
    "\n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "total_tasks = FP + FN + TP + TN\n",
    "accuracy = (TP + TN) / total_tasks if total_tasks > 0 else 0\n",
    "\n",
    "FP_ratio = FP / total_tasks if total_tasks > 0 else 0\n",
    "FN_ratio = FN / total_tasks if total_tasks > 0 else 0\n",
    "TP_ratio = TP / total_tasks if total_tasks > 0 else 0\n",
    "TN_ratio = TN / total_tasks if total_tasks > 0 else 0\n",
    "\n",
    "print(f\"\\nRate:\")\n",
    "\n",
    "print(f\"FP Ratio: {FP_ratio:.2%}\")\n",
    "print(f\"FN Ratio: {FN_ratio:.2%}\")\n",
    "print(f\"TP Ratio: {TP_ratio:.2%}\")\n",
    "print(f\"TN Ratio: {TN_ratio:.2%}\")\n",
    "\n",
    "print(f\"\\nPrecision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Case-wise Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Accuracy:\n",
      "------------------------------------------------------------\n",
      "Total Cases                    Matches    Accuracy  \n",
      "------------------------------------------------------------\n",
      "152                            88         57.89%\n",
      "\n",
      "Accuracy statistics by next value:\n",
      "------------------------------------------------------------\n",
      "Next Value                     Total      Matches    Accuracy  \n",
      "------------------------------------------------------------\n",
      "FINISH                         68         15         22.06%\n",
      "GEOMATERIAL_COLLECTOR          15         15         100.00%\n",
      "HEATMAP_PLOTTER                28         25         89.29%\n",
      "LOCALITY_COLLECTOR             10         8          80.00%\n",
      "NETWORK_PLOTTER                31         25         80.65%\n",
      "\n",
      "Summary - Ongoing vs Finish:\n",
      "------------------------------------------------------------\n",
      "Status                         Total      Matches    Accuracy  \n",
      "------------------------------------------------------------\n",
      "ongoing                        84         73         86.90%\n",
      "finish                         68         15         22.06%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_next_values(record):\n",
    "    \"\"\"\n",
    "    Extract predicted and expected next values from evaluation_details\n",
    "    \n",
    "    Args:\n",
    "        record: Dictionary containing evaluation details\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (predicted_next, expected_next) or (None, None) if not found\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if 'evaluation_details' in record:\n",
    "            details = record['evaluation_details']\n",
    "            predicted_next = details['predicted_next']\n",
    "            expected_next = details['expected_next']\n",
    "            return predicted_next, expected_next\n",
    "        return None, None\n",
    "        \n",
    "    except (KeyError, AttributeError, TypeError):\n",
    "        return None, None\n",
    "\n",
    "def calculate_accuracy_by_next(json_file):\n",
    "    \"\"\"\n",
    "    Calculate accuracy for each unique expected next value\n",
    "    \n",
    "    Args:\n",
    "        json_file (str): Path to the JSON file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (results for each next, results for ongoing vs finish, overall stats)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if not isinstance(data, list):\n",
    "            raise ValueError(\"JSON data should be a list of records\")\n",
    "        \n",
    "        # Initialize counters\n",
    "        stats = defaultdict(lambda: {\"total\": 0, \"matches\": 0})\n",
    "        status_stats = {\n",
    "            \"ongoing\": {\"total\": 0, \"matches\": 0},\n",
    "            \"finish\": {\"total\": 0, \"matches\": 0}\n",
    "        }\n",
    "        overall_stats = {\"total\": 0, \"matches\": 0}\n",
    "        \n",
    "        # Count matches\n",
    "        for record in data:\n",
    "            predicted_next, expected_next = get_next_values(record)\n",
    "            \n",
    "            if expected_next is not None:\n",
    "                # Update overall statistics\n",
    "                overall_stats[\"total\"] += 1\n",
    "                if predicted_next is not None and predicted_next == expected_next:\n",
    "                    overall_stats[\"matches\"] += 1\n",
    "                \n",
    "                # Count for individual next values based on expected_next\n",
    "                stats[expected_next][\"total\"] += 1\n",
    "                if predicted_next is not None and predicted_next == expected_next:\n",
    "                    stats[expected_next][\"matches\"] += 1\n",
    "                \n",
    "                # Count for ongoing vs finish based on expected_next\n",
    "                status = \"finish\" if expected_next.lower() == \"finish\" else \"ongoing\"\n",
    "                status_stats[status][\"total\"] += 1\n",
    "                if predicted_next is not None and predicted_next == expected_next:\n",
    "                    status_stats[status][\"matches\"] += 1\n",
    "        \n",
    "        # Calculate accuracy for each next value\n",
    "        results = {}\n",
    "        for next_value, counts in stats.items():\n",
    "            accuracy = counts[\"matches\"] / counts[\"total\"] if counts[\"total\"] > 0 else 0\n",
    "            results[next_value] = {\n",
    "                \"total\": counts[\"total\"],\n",
    "                \"matches\": counts[\"matches\"],\n",
    "                \"accuracy\": accuracy\n",
    "            }\n",
    "        \n",
    "        # Calculate accuracy for ongoing vs finish\n",
    "        status_results = {}\n",
    "        for status, counts in status_stats.items():\n",
    "            accuracy = counts[\"matches\"] / counts[\"total\"] if counts[\"total\"] > 0 else 0\n",
    "            status_results[status] = {\n",
    "                \"total\": counts[\"total\"],\n",
    "                \"matches\": counts[\"matches\"],\n",
    "                \"accuracy\": accuracy\n",
    "            }\n",
    "        \n",
    "        # Calculate overall accuracy\n",
    "        overall_accuracy = overall_stats[\"matches\"] / overall_stats[\"total\"] if overall_stats[\"total\"] > 0 else 0\n",
    "        overall_results = {\n",
    "            \"total\": overall_stats[\"total\"],\n",
    "            \"matches\": overall_stats[\"matches\"],\n",
    "            \"accuracy\": overall_accuracy\n",
    "        }\n",
    "        \n",
    "        return results, status_results, overall_results\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {json_file} not found\")\n",
    "        return {}, {}, {}\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error: Invalid JSON format\")\n",
    "        return {}, {}, {}\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return {}, {}, {}\n",
    "\n",
    "def main():\n",
    "    json_file = \"evaluated_tasks.json\"\n",
    "    results, status_results, overall_results = calculate_accuracy_by_next(json_file)\n",
    "\n",
    "    # Print overall accuracy first\n",
    "    print(\"\\nOverall Accuracy:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Total Cases':<30} {'Matches':<10} {'Accuracy':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{overall_results['total']:<30} {overall_results['matches']:<10} {overall_results['accuracy']:.2%}\")\n",
    "\n",
    "    # Print detailed results for each next value\n",
    "    print(\"\\nAccuracy statistics by next value:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Next Value':<30} {'Total':<10} {'Matches':<10} {'Accuracy':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    for next_value, stats in sorted(results.items()):\n",
    "        print(f\"{next_value:<30} {stats['total']:<10} {stats['matches']:<10} {stats['accuracy']:.2%}\")\n",
    "\n",
    "    # Print summary for ongoing vs finish\n",
    "    print(\"\\nSummary - Ongoing vs Finish:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Status':<30} {'Total':<10} {'Matches':<10} {'Accuracy':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    for status, stats in status_results.items():\n",
    "        print(f\"{status:<30} {stats['total']:<10} {stats['matches']:<10} {stats['accuracy']:.2%}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
